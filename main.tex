\documentclass[fleqn,10pt]{wlscirep}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{subcaption}
\usepackage{xargs}  % Use more than one optional parameter in a new commands
\usepackage[colorinlistoftodos,prependcaption]{todonotes}

\graphicspath{{figures/}}
\sisetup{locale=US,group-minimum-digits=5,group-separator={,}}

\newcommandx{\alltodo}[2][1=]{\todo[author=Everyone,linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\mctodo}[2][1=]{\todo[author=Matt,linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\rokemtodo}[2][1=]{\todo[author=Ariel,linecolor=green,backgroundcolor=green!25,bordercolor=green,#1]{#2}}
\newcommandx{\comment}[2][1=]{\todo[author=Comment,linecolor=lime,backgroundcolor=lime!25,bordercolor=lime,#1]{#2}}
\newcommandx{\arhtodo}[2][1=]{\todo[author=Adam,linecolor=purple,backgroundcolor=purple!25,bordercolor=purple,#1]{#2}}

\title{A preprocessed open diffusion derivatives dataset for child and adolescent neuroimaging}

\author[1,*$\dagger$]{Adam Richie-Halford}
\author[2,$\dagger$]{Matthew Cieslak}
\author[4]{Lei Ai}
\author[5]{Sendy Caffarra}
\author[4]{Alexandre R. Franco}
\author[5]{Iliana Karipidis}
\author[3]{John Kruper}
\author[4]{Michael Milham}
\author[5]{Barbara Avelar Pereira}
\author[5]{Ethan Roy}
\author[2]{Valerie J. Sydnor}
\author[5]{Jason Yeatman}
\author[6]{The Fibr Community Science Consortium}
% \input{fibr_authors.tex}
\author[2,$\ddagger$]{Theodore D. Satterthwaite}
\author[3,1,$\ddagger$]{Ariel Rokem}

\affil[1]{University of Washington, eScience Institute, Seattle, Washington, 98195, USA}
\affil[2]{University of Pennsylvania, Department of Psychiatry, Philadelphia, Pennsylvania, 19104, USA}
\affil[3]{University of Washington, Department of Psychology, Seattle, Washington, 98195, USA}
\affil[4]{Child Mind Institute, New York City, 10022, USA}
\affil[5]{Stanford University, Graduate School of Education and Division of Developmental and Behavioral Pediatrics, Stanford, California, 94305, USA}
\affil[6]{The Fibr Community Science Consortium}

\affil[*]{richford@uw.edu}
\affil[$\dagger$]{these authors contributed equally to this work}
\affil[$\ddagger$]{these authors contributed equally to this work}

%\keywords{Keyword1, Keyword2, Keyword3}

\begin{abstract}
The Healthy Brain Network (HBN) is a landmark pediatric mental health study.
Among other data, the HBN collects diffusion MRI (dMRI) from its participants,
used to make inferences about brain connections. We curated the
currently-available dMRI data from \num{2747} HBN subjects into the widely
accepted Brain Imaging Data Structure (BIDS) format, we processed the data
through a series of standard steps, to denoise the data and correct it for
motion and eddy current effects. To perform quality control (QC) on this large
dataset, we deployed a community science app. Together with a small subset of
expert QC data, community science QC scores provided the information needed to
train an artificial neural network that scores the data for quality in a manner
that highly agrees with expert QC. We demonstrate that the neural network uses
sensible features of the data and also demonstrate that accurate quality control
has implications for subsequent analysis of this data. We openly released the
processed derivatives and QC scores for use by the scientific community.
\end{abstract}

\begin{document}

\flushbottom
\maketitle
\thispagestyle{empty}

\todo[inline]{%
    Note to editing authors:

    We're targeting an eLife ``Tools and Resources'' paper for first submission,
    but we have used the Scientific Data \LaTeX template. For Scientific data,
    the abstract max is 170 words. eLife has no set max.

    We use the \texttt{todonotes} package to keep track of remaining tasks and
    comments. You can add a task for Adam with the
    \texttt{\textbackslash{}arhtodo} command, a task for Matt with the
    \texttt{\textbackslash{}mctodo} command, a task for Ariel with the
    \texttt{\textbackslash{}rokemtodo} command, a task for all reviewing authors
    with the \texttt{\textbackslash{}alltodo} command, and a general comment
    with the \texttt{\textbackslash{}comment} command.
}
\comment[inline]{Comments look like this.}
\arhtodo[inline]{Tasks for Adam look like this.}
\rokemtodo[inline]{Tasks for Ariel look like this.}
\mctodo[inline]{Tasks for Matt look like this.}
\alltodo[inline]{Tasks for all reviewing authors look like this.}

\section*{Introduction}

Childhood and adolescence are marked by rapid dynamic change to human brain
structure and function \cite{Lebel2018-oy}. It is also a time during which the
symptoms of many mental health disorders tend to emerge \cite{Paus2008-gi}.
Understanding individual differences in brain development and understanding the
interplay between brain development and the development of these disorders
requires measurements from many invidividuals \cite{Paus2010-qk, Fair2021-eg}.

The Healthy Brain Network (HBN) is a landmark pediatric mental health study
collecting MRI images and clinical assessment data from \num{10000} New York
City area children and adolescents \cite{alexander2017-yc}. The HBN dataset
takes a transdiagnostic approach, measuring a broad range of phenotypic and
imaging data in each individual. One of the brain imaging measurements conducted
in every individual is diffusion MRI (dMRI), which allows for analysis of the
physical properties of developing white matter \cite{wandell2016-qt}. The dMRI
data is openly available in its raw form through the Functional Connectomes
Project and the International Neuroimaging Data-Sharing Initiative (FCP-INDI),
spurring collaboration on open big-data reproducible science
\cite{avesani2019-ey}. However, even with the data publicly available, several
steps need to be completed before the data can be fruitfully analyzed.

First, analysis of dMRI data must start with a pipeline of critical
preprocessing steps, such as eddy current correction, motion correction, and
adjustment of the gradient directions. Because of the complexity of some of
these steps, investigators may neglect to perform some preprocessing steps or
may make errors that can induce bias in their subsequent interpretation of the
data \cite{jones2010-ps}. Furthermore, once preprocessing is done correctly and
transparently once, there is little need for researchers to repeat this step.
Thus, there is a need for an openly available preprocessed diffusion derivative
dataset that applies best practices in preprocessing in a robust and transparent
way. Here, we used \emph{QSIPrep}, an integrative platform for preprocessing and
reconstructing diffusion MRI data, which implements these best practices
\cite{cieslak2021-iq}. To find the relevant files within a dataset, QSIPrep
relies on the Brain Imaging Data Specification (BIDS) standard
\cite{gorgolewski2016-lh}, which describes the way to organize neuroimaging data
for automated processing. That is, it is a BIDS App \cite{Gorgolewski2017-mb}.
To reach full compliance with the BIDS standard, data in HBN had to first be
curated.

\mctodo[inline]{maybe we need a couple more words here to say what that means?}

Second, dMRI measurements are susceptible to a variety of artifacts that affect
the quality of the signals and the ability to make accurate inferences from
them. In small studies, with few participants, it is common to thoroughly
examine the data from every participant as part of a quality control (QC)
process. Expert examination is time consuming and is prohibitive in large
datasets, such as HBN. This difficulty could be ameliorated through the
automation of QC. Thanks to their success in many other visual recognition
tasks, previously only achievable by humans, machine learning and computer
vision methods, such as convolutional deep artificial neural networks, or ``deep
learning'' \cite{lecun2015deep} are promising avenues for automation of QC. One
of the challenges of these new methods is that they require a large training
dataset to attain accurate performance. In previous work, we demonstrated that
deep learning can accurately emulate expert examination of T1-weighted
(T1w)brain images for QC \cite{keshavan2019-er}. To obtain a large enough
training dataset of T1w images in that case, we deployed a community science
application that collected quality control scores of parts of the dataset from
volunteers through a web application \footnote{%
    While the legacy term ``citizen science'' evokes a sense of civic duty in
    scientific engagement, it can also imply a barrier for community members who
    want to contribute to science but may not be citizens of a particular
    country. In this manuscript we use the more modern term ``community
    science.''
}.
These scores were then calibrated using a gold standard expert-scored subset of
these images. A deep learning neural network was trained on the calibrated and
aggregated score, resulting in very high concordance with expert ratings on a
separate test dataset. We term this approach ``hybrid QC'', because it combines
information from experts with information from community scientists to create a
scalable machine learning algorithm that can be applied to future data
collection.

However, the hybrid QC proof-of-concept left lingering questions about its
applicability to other datasets because it was trained on a single-site,
single-modality dataset. Here, we expand the hybrid-QC approach to a large
multi-site diffusion MRI dataset. Moreover, one of the common critiques of deep
learning is that it can learn irrelevant features of the data and does not
provide information that is transparent enough to interpret
\cite{lipton2017doctor, salahuddin2022transparency, Zech2018-ki}. To confirm
that the hybrid-QC deep learning algorithm uses meaningful features of the
diffusion-weighted images to perform accurate QC, we used machine learning
interpretation methods that pry open the ``black box'' of the neural network
highlighting the features that lead to a specific QC score
\cite{sundararajan2017axiomatic,murdoch2019definitions}.

Taken together, BIDS-curated data, processed data, and quality control scores
generated by the deep learning algorithm provide researchers with everything
that they need in order to start applying a large variety of analysis approaches
to these HBN Preprocessed Open Diffusion Derivatives (HBN-POD2), and answer the
questions that this data was collected to answer.

\section*{Results}

The aims of this study were fourfold
\begin{enumerate*}[%
    label=(\roman*),%
    before=\unskip{: },%
    itemjoin={{, }},%
    itemjoin*={{, and }}]
    \item curate the HBN MRI data into a fully-BIDS compliant MRI dataset
    \item perform state-of-the-art diffusion MRI (dMRI) preprocessing using \emph{QSIPrep}
    \item assign QC scores to each subject
    \item provide unrestricted public release of the outputs from each of these
    steps.
\end{enumerate*}

Starting with MRI data from \num{2747} HBN participants available on FCP-INDI,
we curated these data for compliance with the Brain Imaging Data Structure
(BIDS) specification \cite{gorgolewski2016-lh} and preprocessed the structural
MRI (sMRI) and diffusion MRI (dMRI) data using \emph{QSIPrep}. Subjects that
could not be curated to comply with the BIDS standard or that did not have dMRI
data were excluded, resulting in \num{2136} subjects with preprocessed,
BIDS-compliant dMRI data (Figure~\ref{fig:hbn-sankey}).

\comment{AR: I wonder whether we want some kind of characterization of the data before we jump into the QC part. For example, distributions of the statistics displayed in Figure 2(a) across the full sample, to give a sense. Maybe their correlation with age? We come back to that later on, so this might be useful as a foreshadowing of the QC vs. age plot in Figure 4(b)}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\linewidth]{hbn-pod2-sankey.png}
    \caption{%
        {\bf HBN-POD2 data provenance}:
        Imaging data for \num{2747} subjects, aged 5-21 years and collected at four
        sites in the New York City area, was made available through the
        Functional Connectomes Project and the International Neuroimaging
        Data-Sharing Initiative (FCP-INDI).
        %
        These data were curated for compliance to the BIDS specification
        \cite{gorgolewski2016-lh} and availability of imaging metadata in json
        format. \num{2615} subjects met this specification.
        %
        Imaging data was preprocessed using \emph{QSIPrep} \cite{cieslak2021-iq}
        to group, distortion correct, motion correct, denoise, coregister and
        resample MRI scans. Of the BIDS curated subjects, \num{2136} subjects
        passed this step, with the majority of failures coming from subjects
        with missing dMRI scans.
        %
        Expert raters assigned QC scores to \num{200} of these participants,
        creating a ``gold standard'' QC subset. Community raters then assigned
        binary QC ratings to a superset of the gold standard containing
        \num{1653} participants. An image classification algorithm was trained
        on a combination of automated QC metrics from QSIPrep and community
        scientist reviews to ``extend'' the expert ratings to the community
        science subset. Finally, a deep learning QC model was trained on the
        community science subset to assign QC scores to the entire dataset and
        to future releases from HBN.
        %
        The HBN-POD2 dataset, including QC ratings, is openly available through
        FCP-INDI.
    }
    \label{fig:hbn-sankey}
\end{figure}

To achieve the QC aim of the project, we adopted a hybrid QC approach that
combines expert rating, community science, and deep learning, drawing on the
success of a previous application in assessing the quality of HBN's structural
MRI data \cite{keshavan2019-er}. This method
\begin{enumerate*}[%
    label=(\roman*),%
    before={{ }},%
    itemjoin={{, }},%
    itemjoin*={{ and }}]
    \item starts with dMRI expert raters labelling a small subset of subjects,
    the ``gold standard'' dataset
    \item amplifies these labels using a community science web application to
    extend expert ratings to a much larger subset of the data, the community
    science subset
    \item trains a deep learning model on the community science subset to
    predict expert decisions on the entire dataset.
\end{enumerate*}

\subsection*{Expert quality control}

To create a gold standard QC dataset, we first developed \emph{dmriprep-viewer},
a dMRI data viewer and QC rating web application to display \emph{QSIPrep}
outputs and collect expert ratings \cite{richie-halford2021-viewer}. Six of the
co-authors, who are all dMRI experts, rated a 200-participant subset of the
HBN-POD2 data using extensive visual examination of each subjects dMRI data,
including the diffusion weighting imaging (DWI) time series itself, a plot of
motion parameters through the DWI scan, and full 3D volumes depicting
\begin{enumerate*}[%
    label=(\roman*),%
    before={{ }},%
    itemjoin={{, }},%
    itemjoin*={{ and }}]
    \item the brain mask and $b=0$ to T1w registration
    \item a directionally encoded color FA (DEC-FA) image laid over the $b=0$ volume.
\end{enumerate*}
The experts rated subjects using a five-point scale with ratings of ``definitely
fail,'' ``probably fail,'' ``unsure,'' ``probably pass,'' and ``definitely
pass.'' The distribution of scores given by the experts demonstrates that the
gold standard dataset included a range of data quality. Mean expert ratings
correlate with some of the automated QC metrics output by \emph{QSIPrep}:
neighboring DWI correlation (NDC) \cite{yeh2019-kb}, maximum relative
translation, and number of outlier slices (Pearson CC: XXX). NDC characterizes
the pairwise spatial correlation between pairs of DWI volumes that sample
neighboring points in $q$-space. Since lower values indicate reduced data
quality, it is reassuring that NDC correlates directly with expert rating
(Pearson CC: XXX). Conversely, high relative translation and a high number of
motion outlier slices reflect poor data quality and these metrics are inversely
related to mean expert rating (Pearson CC: XXX and Pearson CC: XXX,
respectively).

In addition to agreeing qualitatively with \emph{QSIPrep}'s automated QC metrics
on average, the expert raters also tended to agree with each other. We assessed
inter-rater reliability (IRR) using the pairwise Cohen's $\kappa$, which
\cite{di-eugenio2004-bb} exceeds 0.52 in all cases, and with a mean value of
0.648. In addition to the pairwise Cohen's $\kappa$, we also computed the
intra-class correlation (ICC) \cite{hallgren2012-ze} as a measure of IRR. ICC3k
is the appropriate variant of the ICC to use when a fixed set of $k$ raters each
code an identical set of subjects, as is the case here. ICC3k for inter-rater
reliability among the experts was 0.930 (95\% CI: [0.91, 0.94]), which is
qualitatively considered an ``excellent'' level of IRR \cite{Cicchetti1994-fz}.
The high IRR provides confidence that the average of the expert ratings for each
image in the gold standard is an appropriate target to use for training a
machine learning model that predicts the expert scores.


\begin{figure}[ht]
    \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{community-qc/expert-qsiprep-pairplot-top.pdf}
    \includegraphics[width=\linewidth]{community-qc/expert-qsiprep-pairplot-bottom.pdf}
    \caption{}
    \label{fig:expert-qc:scatter}
    \end{subfigure}
    \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{community-qc/expert-raters-cohens-kappa.pdf}
    \caption{}
    \label{fig:expert-qc:irr}
    \end{subfigure}
    \caption{%
        {\bf Expert QC results}:
        Six dMRI experts rated a subset of \num{200} subjects.
        \textbf{(a)} Experts agree with \emph{QSIPrep}'s automated QC metrics.
        Here we show associations between the expert QC rating and the
        \emph{QSIPrep} metrics neighboring diffusion-weighted imaging
        correlation (NDC) \cite{yeh2019-kb}, maximum relative translation, and
        number of outlier slices. As expected, NDC is directly correlated with expert rating while
        the other two metrics are inversely correlated with expert rating.
        %
        \textbf{(b)} Experts agree with each other. Here we show the pairwise
        Cohen's $\kappa$ measure of inter-rater reliability (see text for ICC
        calculations). The XGB model has an inter-rater reliability (quantified
        here as Cohen's $\kappa$) that is indistinguishable from the other
        raters
    }
    \label{fig:expert-qc}
\end{figure}

\subsection*{Community science quality control}

\arhtodo{What is the number instead of XXX below?}

Although the expert raters achieved high IRR and yielded intuitive associations
with \emph{QSIPrep}'s automated QC metrics, generating expert QC labels for the
entire HBN-POD2 dataset would be prohibitively time consuming. To assess the
image quality of the remaining subjects, we deployed \emph{Fibr}
(\url{https://fibr.dev}), a community science web application in which users
assign binary pass/fail labels assessing the quality of horizontal slice DEC-FA
images overlaid on the $b=0$ image. That is, \emph{Fibr} users saw individual
slices or an animated sequence of XXX slices taken from the entire DEC-FA volume
that the expert raters saw. The \emph{Fibr} users therefore saw only a subset of
the imaging data that the dMRI experts had access to for each subject, but they
saw many more subjects. In total, \num{374} community scientists provided
\num{587778} ratings for a mean of $>50$ ratings per slice (or $>200$ ratings
per subject) from \num{1653} subjects. Of the community scientists, \num{145}
raters provided $>3,000$ ratings each and are included in the Fibr Community
Science Consortium as co-authors on this paper.

The unadjusted \emph{Fibr} ratings are overly optimistic; i.e., on average,
community scientists are not as critical as the expert raters (Figure
\ref{fig:fibr-qc:scatter}a). In addition, different community scientists
probably provide data of differing accuracy. In addition, additional information
about quality is provided in the \emph{QSIPrep} QC metrics. To account for this
variability, and take advantage of the information provided by \emph{QSIPrep},
we trained a gradient boosting model \cite{chen2016-eb} to predict expert scores
based on a combination of community science ratings and automated \emph{QSIPrep}
QC metrics. We refer to this model as XGB.

To clarify the contributions of the automated QC metrics and the community
science raters, we trained two additional gradient boosting models
\begin{enumerate*}[%
    label=(\roman*),%
    before=\unskip{: },%
    itemjoin={{, }},%
    itemjoin*={{ and }}]
    \item one trained only on the automated \emph{QSIPrep} QC metrics, which we
    call XGB-q
    \item one trained on only the \emph{Fibr} ratings, which we call XGB-f.
\end{enumerate*}

XGB-f may be viewed as a data-driven weighting of community scientists' ratings,
while XGB-q may be viewed as a generalization of QC metric exclusion criteria.
XGB, combining information from both \emph{Fibr} ratings and \emph{QSIPrep} QC
metric attained a cross-validated area under the receiver operating curve
(ROC-AUC) of $0.96 \pm 0.01$ on the ``gold standard,'' where the $\pm$ indicates
the standard deviation of scores from repeated $k$-fold cross-validation
((Figure \ref{fig:fibr-qc:scatter}a)). In contrast, XGB-q attained an ROC-AUC of
$0.91 \pm 0.03$ and XGB-f achieved an ROC-AUC of $0.84 \pm 0.04$. The enhanced
performance of XGB-q over XGB-f shows that community scientists alone are not as
accurate as automated QC metrics are at predicting expert ratings. And yet, the
increased performance of XGB over XGB-q demonstrates that there is additional
image quality information to be gained by incorporating community scientist input.

As a way of evaluating the quality of the XGB predictions, consider the fact
that the average Cohen's $kappa$ between XGB and the expert raters is 0.74,
which is higher than the average Cohen's $kappa$ between any of the other raters
and their human peers (Figure~\ref{fig:expert-qc}). This is not suprising,
given that the XGB model was fit to optimize this match, but further
demonstrates the goodness of fit of this model.

Nevertheless, this provides confidence in using the XGB scores in the next step
of analysis. When a subset of subjects is coded by multiple raters and the
reliability of their ratings is meant to generalize to other subjects rated by
only one coder, the single-measure ICC3 must be used. When adding XGB to the
existing expert raters as a seventh expert, $\textbf{ICC3} = 0.709 (95\% CI:
[0.66, 0.75])$. This high ICC3 value after inclusion of the XGB model justifies
using the XGB scores as the target for training an image-based deep learning
network.

\arhtodo{Each figure caption should explain what the error bars mean. In particular, in Figure 3b}

\begin{figure}[tbp]
    \begin{subfigure}{.55\textwidth}
    \centering
    \includegraphics[width=\linewidth]{community-qc/fibr-rating-scatter-plot.pdf}
    \caption{}
    \label{fig:fibr-qc:scatter}
    \end{subfigure}
    \begin{subfigure}{.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{community-qc/xgb-roc-curve.pdf}
    \caption{}
    \label{fig:fibr-qc:roc}
    \end{subfigure}
    \caption{%
        {\bf Community science predictions of the expert ratings}:
        \textbf{(a)} Scatterplots showing the relationship between mean expert
        rating and both mean \emph{Fibr} rating (left) and XGB prediction
        (right). \emph{Fibr} raters overestimate the quality of images compared
        to expert raters. But the XGB prediction compensates for this by
        incorporating automated QC metrics and weighting more valuable
        \emph{Fibr} raters.
        %
        \textbf{(b)} ROC curves for the XGB, XGB-q, and XGB-f models.
    }
    \label{fig:fibr-qc}
\end{figure}

\subsection*{Automated quality control labelling through deep learning}

The XGB ``rater'' does a good job of extending QC ratings to the entire
community science subset, but requires \emph{Fibr} scores. Without community
science ratings, we would have to fall back on the less accurate XGB-q
prediction. Instead, we would like a fully automated QC approach that can be
readily applied to new data releases from HBN.

We therefore trained a deep convolutional neural network to predict the XGB
ratings directly from \emph{QSIPrep} outputs. We modified an existing 3D
convolutional neural network (CNN) architecture \cite{zunair2020-bs}, previously
applied to the ImageCLEF Tuberculosis Severity Assessment 2019 benchmark
\cite{dicente2019clef} to accept multichannel input generated from the
preprocessed dMRI: the $b=0$ reference diffusion image, each of the three
cardinal axis components of the DEC-FA image, and, optionally, automated QC
metrics from QSIPrep. We trained this network on XGB scores and validated it
against the gold standard expert-scored dataset. We refer to the convolutional
neural network model trained only on imaging data as CNN-i and the model that
incorporates automated QC metrics as CNN-i+q. The two models performed nearly
identically and achieved an ROC-AUC of $0.947$ (Figure \ref{fig:dl-qc:roc}). The
near-identical performance suggests that \emph{QSIPrep}'s automated QC metrics
provide information that is redundant with information available in the imaging
data. Both CNN-i and CNN-i+q outperformed XGB-q, which was trained only on
automated QC metrics, but both underperformed relative to the full XGB model,
that uses \emph{Fibr} scores in addition to the \emph{QSIPrep} QC scores.

\begin{figure}[htbp]
    \begin{subfigure}{.25\textwidth}
    \centering
    \includegraphics[width=\linewidth]{deep-learning-qc/dl_roc_auc_curve.pdf}
    \caption{}
    \label{fig:dl-qc:roc}
    \end{subfigure}
    \begin{subfigure}{.25\textwidth}
    \centering
    \includegraphics[width=\linewidth]{bundle-profiles/qc-age-jointplot.pdf}
    \caption{}
    \label{fig:dl-qc:joint}
    \end{subfigure}
    \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{bundle-profiles/qc-hist.pdf}
    \caption{}
    \label{fig:dl-qc:hist}
    \end{subfigure}
    \caption{%
        {\bf Deep learning QC scores}:
        \textbf{(a)} ROC curves for two deep learning models: one trained with
        additional automated QC metrics from \emph{QSIPrep} (blue) and one
        trained without (orange). The models performed roughly identically,
        indicating the the QC metrics provide information that is redundant to
        the imaging. Both outperformed the XGB-q predictions, indicating the
        added value of the diffusion weighted images. However, both models
        underperformed the XGB predictions, which also incoporate
        information from \emph{Fibr} ratings for each scan.
        %
        \textbf{(b)} Joint distributions showing a strong direct association
        between age and QC score. This likely reflects the well-known negative
        association between age and head motion in pediatric neuroimaging.
        %
        \textbf{(c)}
        Histograms showing the relationship between participants QC scores and
        their sex (left) and scan site (right). QC distributions are independent
        of sex and scanning site
    }
    \label{fig:dl-qc}
\end{figure}

While HBN-POD2 provides four QC ratings: the mean expert QC ratings, XGB-q and
XGB predicted scores, as well as the CNN-i predicted score, we treat the CNN-i
score as the definitive QC score because it is available for all participants,
can be easily calculated for new participants in future HBN releases and is more
accurate than XGB-q. When we refer to a participant's QC score without
specifying a generating model, the CNN-i score is assumed. Figure
\ref{fig:dl-qc} depicts the distribution of these QC scores by age (Figure
\ref{fig:dl-qc:joint}), sex, and scanning site (Figure \ref{fig:dl-qc:hist}).
QC distributions are similar for each scan site and for male and female
participants \footnote{%
    Responses for the sex variable in HBN phenotypic data are limited to
    ``male'' and ``female.''
}. Unsurprisingly, the QC score is strongly directly correlated with age. This
accords with the negative association between head motion and age, which is well
established both in general \cite{power2012spurious,satterthwaite2012impact,fair2012distinct,yendiki2014spurious} and specifically in the HBN dataset \cite{alexander2017-yc}.

\subsection*{Attribution masks for the deep learning classifier}

The predictive performance of the CNN-i model (Figure \ref{fig:dl-qc:roc}) gives
us confidence that it could accurately classify unseen data, justifying its
extension to the entire HBN-POD2 dataset and to future releases. However, it
does not explain its decisions. As deep learning models have been increasingly
applied to medical image analysis, there is an evolving interest in the
transparency of these models \cite{salahuddin2022transparency}. While an
exhaustive interpretation of deep learning QC models is beyond the scope of this
work, we provide a preliminary qualitative interpretation of the CNN-i model.

We generated post-hoc attribution maps that highlight regions of the input
volume that are relevant for the QC score. The integrated gradient method
\cite{sundararajan2017axiomatic} is a gradient-based attribution method
\cite{ancona2019gradient} that aggregates gradients for synthetic images
interpolating between a baseline image and the input image. It has been used to
interpret deep learning models applied to retinal imaging in diabetic
retinopathy \cite{sayres2019using} and glaucoma \cite{Mehta2021-zp} prediction,
as well as in multiple sclerosis prediction from brain MRI
\cite{wargnier-dauchelle2021interpretable}. Our goal is to confirm that the
CNN-i model is looking at the same features that an expert rater would look at,
thereby bolstering the decision to apply it to new data.

Figure \ref{fig:ig} shows attribution maps for example participants from each
confusion class: true positive, true negative, false positive, and false
negative. The columns correspond to the different channels of the deep learning
input volume: the $b=0$ reference image and the DEC-FA in the $x$, $y$, and $z$
directions. The true positive map indicates that the network is looking at the
entire brain rather than focusing on any one anatomical region (Figure
\ref{fig:ig:true-pos}). Moreover, the model identifies white matter fascicles
that travel along the direction of the input channel: lateral for $x$,
anterior-posterior for $y$, and superior-inferior for $z$. The true negative
attribution map (Figure \ref{fig:ig:true-neg}) reveals that when the reference
$b=0$ volume contains motion artifacts, such as banding, the network ignores the
otherwise positive attributions for the clearly identifiable white matter tracts
in the DEC-FA channels. The false positive map (Figure \ref{fig:ig:false-pos})
and the false negative map (Figure \ref{fig:ig:false-neg}) should be interpreted
differently since they come from low confidence predictions; the probability of
passing hovered on either side of the pass/fail threshold. For example, in the
false positive case, the network is confused enough that it treats voxels that
are outside of the brain as informative as voxels in the major white matter
bundles.

\begin{figure}[tbp]
    \centering
    \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{deep-learning-qc/attribution-maps-true-pos.pdf}
    \caption{true positive}
    \label{fig:ig:true-pos}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{deep-learning-qc/attribution-maps-true-neg.pdf}
    \caption{true negative}
    \label{fig:ig:true-neg}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{deep-learning-qc/attribution-maps-false-pos.pdf}
    \caption{false positive}
    \label{fig:ig:false-pos}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{deep-learning-qc/attribution-maps-false-neg.pdf}
    \caption{false negative}
    \label{fig:ig:false-neg}
    \end{subfigure}
    \caption{%
        {\bf Integrated gradients attribution maps for the deep learning classifier}:
        Each column depicts a different channel of the input tensor: the $b=0$
        DWI volume and the DEC-FA images in the $x$, $y$, and $z$ directions.
        The first three columns show a horizontal slice while the last column
        shows a coronal slice. Blue voxels indicate positive attribution (i.e.
        evidence for passing the subject), while red voxels indicate negative
        attribution (i.e. evidence for QC failure). The underlying greyscale
        depicts the input channel. Each row depicts a representative subject
        from each confusion class:
        %
        \textbf{(a)} Attribution maps for a true positive prediction. The model
        looks at the entire brain and focuses on known white matter bundles in
        the DEC-FA channels. In particular, it focuses on lateral bundles in the
        $x$ direction, anterior-posterior bundles in the $y$ direction, and
        superior-inferior bundles in the $z$ direction.
        %
        \textbf{(b)} Attribution maps for a true negative prediction. The model
        focuses primarily on the $b=0$ channel, suggesting that it ignores
        DEC-FA when motion artifacts like banding are present.
        %
        \textbf{(c)} Attribution maps for a false positive prediction. Both the
        false positive and negative predictions were low confidence predictions.
        This is reinforced by the fact that the model views some voxels that are
        outside of the brain as just as informative as those in major white
        matter tracts.
        %
        \textbf{(d)} Attribution maps for a false negative prediction. The model
        fails to find long-range white matter tracts in the anterior-posterior
        and lateral directions. We also speculate that the model expects
        left-right symmetry in the DEC-FA channels and assigns negative
        attribution to asymmetrical features. }
    \label{fig:ig}
\end{figure}

\subsection*{Quality control improves inference}

To demonstrate the effect that quality control has on inference, we analyzed
tract profile data derived from HBN-POD2 data. Tract profiling
\cite{yeatman2012-rc,jones2005pasta,colby2012along,odonnell2009tract,
kruper2021evaluating} is a subset of tractometry
\cite{jones2005pasta,bells2011tractometry}, which uses the results of dMRI
tractography to quantify properties of the white matter along major pathways.
Tract-profiling retains the values of diffusion metrics along the trajectory of
each bundle of tractography streamlines, rather than computing summary
statistics summarized at the level of each bundle. In Figure
\ref{fig:qc-profiles:md}, we plot mean diffusivity tract profiles grouped into
four QC bins along the length of twenty-four bundles: While some bundles, such
as the cingulum cingulate (CGC) and the inferior longitudinal fasciculus (ILF),
appear insensitive to QC score, others, such as the uncinate (UNC) and the
orbital portion of the corpus callosum, exhibit strong differences between QC
bins. In most bundles, low QC scores tend to flatten the profile.


\begin{figure}[tbp]
    \centering
    \includegraphics[width=\linewidth]{bundle-profiles/qc-bins-dki-md.pdf}
    \caption{%
        {\bf MD bundle profiles show large QC group differences}:
        MD profiles binned by QC score in twenty-four major while matter
        bundles. The left and right uncinate bundles are the most sensitive
        to QC score. Generally, QC score tends to flatten bundle profiles.
        Error bands represent 95\% confidence intervals. Bundle abbreviations
        for lateralized bundles contain a trailing ``L'' or ``R'' indicating the
        hemisphere. Bundle abbreviations:
        inferior fronto-occipital fasciculus (IFO),
        uncinate (UNC),
        thalamic radiation (ATR),
        corticospinal (CST),
        arcuate (ARC),
        superior longitudinal fasciculus (SLF).
        inferior longitudinal fasciculus (ILF),
        cingulum cingulate (CGC),
        orbital corpus callosum (Orbital),
        anterior frontal corpus callosum (AntFrontal),
        superior frontal corpus callosum (SupFrontal),
        motor corpus callosum (Motor),
        superior parietal corpus callosum (SupParietal),
        temporal corpus callosum (Temporal),
        post-parietal corpus callosum (PostParietal), and
        occipital corpus callosum (Occipital).
    }
    \label{fig:qc-profiles:md}
\end{figure}

\arhtodo[inline]{
    The tract profile plot needs improvement. Specifically, the axes text
    takes up a lot of room so it's hard to see the plots.
}

The effect of QC score on white matter bundle profiles suggests that researchers
using HBN-POD2 incorporate QC in their analyses, either by applying a QC cutoff
when selecting subjects or by explicitly added QC score to their inferential
models. Failure to do so may cause spurious associations or degrade predictive
performance. To demonstrate this, we select participant age as a representative
phenotypic benchmark \cite{cole2019brain,richie-halford2021multidimensional} and
observed the effect of varying QC cutoff on the predictive performance of an age
prediction model. Cross-validated $R^2$ scores for an age prediction model
varies depending on the QC cutoff (Figure~\ref{fig:age-prediction}). The
increase in model performance from imposing a QC cutoff is intuitive: we know
from Figure~\ref{fig:qc-profiles:md} that participants with low QC scores have
reduced MD, but MD also decreases as subjects mature
\cite{yeatman2014lifespan,richie-halford2021multidimensional}. Eliminating
subjects with low QC therefore removes subjects who may look artificially older
from the analysis, improving overall performance. The most noticeable
improvement in performance comes after imposing the most modest cutoff of
$0.05$, suggesting that researchers will benefit from \emph{any} QC screening.
On the other hand, QC screening inherently introduces a tension between the
desire for high quality data and the desire for a large sample size. In this
case, after a QC cutoff of around $0.9$, the training set size is reduced such
that it degrades predictive performance. We do not expect the sensitivity
analysis of an age prediction model to generalize to other analyses and
recommend that researchers using HBN-POD2 conduct their own.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\linewidth]{age-prediction/qc_sweep.pdf}
    \caption{%
        {\bf Imposing a QC cutoff improves age prediction}:
        Cross validated $R^2$ scores (left axis, blue dots) from an age
        prediction model increase after screening subjects by QC score. We see
        the most dramatic increase in $R^2$ after imposing even the lowest
        cutoff of $0.05$. Thereafter, the $R^2$ scores trend upward until a
        cutoff of $\sim 0.95$, where the training set size (right axis, orange
        line) becomes too small to sustain model performance.
    }
    \label{fig:age-prediction}
\end{figure}

\section*{Discussion}

We present HBN-POD2, one of the largest child and adolescent diffusion imaging
datasets with derived measures that is currently openly available. The dataset
was designed to comply with the best practices of the field. For example, it
complies with the current draft of the BIDS diffusion derivative specification
\cite{Pestilli2021}. It will grow continuously as the HBN study acquires more
data, eventually reaching its \num{10000} subject goal. Here we summarize the
contributions of this work, starting from the potential impact of the range of
resources we have created and ending with implications for processing and
quality control of other large-scale datasets.

\subsection*{Standardized preprocessing and QC increases the impact of openly-available data}

The most immediate contribution of this work is a large analysis-ready dMRI
dataset, openly accessible to the public. In the past decade, projects such as
the Human Connectome Project (HCP) \cite{van-essen2013-oi}, UK Biobank
\cite{miller2016-mq}, ABCD \cite{jernigan2018-my}, and CamCAN
\cite{taylor2017-or,shafto2014-ld} and of course FCP-INDI (which includeds HBN)
\cite{Mennes2013-eu} have ushered a culture of data sharing in open big-data
neuroscience. The adoption and reuse of these datasets reduces or eliminates the
data collection burden on downstream researchers. Some projects, such as the HCP
\cite{glasser2013-lo}, also provide preprocessed derivatives, further reducing
researchers' burden and extending the benefits of data-sharing from data
collection to preprocessing and secondary analysis. Following the example of the
HCP, HBN-POD2 provides analysis-ready dMRI derivatives. This avoids duplication
of preprocessing effort while also ensuring a minimum standard of data quality
for HBN researchers. The data is amenable to many different analyses, including
tractometry \cite{yeatman2012-rc}, graph theoretical analysis \cite{yeh2020-nu},
and combinations with functional MRI data and other data types for the same
subjects. The availability of standardized preprocessed diffusion data will
allow researchers to create and test hypotheses on the white matter properties
underlying behavior and disease, from reading and math acquisition to childhood
adversity and mental health. As such, this dataset will accelerate discovery at
the nexus of structural connectivity and neurodevelopmental and learning
disorders.

In this kind of study, it is crucial to perform accurate and reliable QC of the
data. We provide QC scores for the subjects that we have preprocessed, paving
the way for others to incorporate considerations of data quality into their
analysis of the processed data. Importantly, the deep learning network
architecture for QC and parameters are also provided as part of this work,
allowing application of this network to future releases of HBN data, and
allowing other researchers to build upon and improve upon this work. Moreover,
undertaking this processing and QC effort required construction and deployment
of substantial informatics infrastructure, including tools for cloud computing,
web applications for expert annotation and for community science rating and
analysis software. All of these tools are provided openly, so that this approach
can be adopted widely in other projects and in other scientific fields.

\subsection*{Fully transparent pipelines provide an extensible baseline for future methods development}

While the primary audience of HBN-POD2 is researchers in neurodevelopment who
will use the dMRI derivatives in their studies, other researchers may use
HBN-POD2 to develop new preprocessing algorithms or quality control methods. In
this respect, HBN-POD2 follows Avesani et al.~\cite{avesani2019-ey}, who
recognized the diverse interests that different scientific communities have in
reusing neuroimaging data and coined the term \emph{data upcycling} to promote
multiple-use data sharing for purposes secondary to those of the original
project. Complementing the open diffusion data derivatives in Avesani et al.'s
work, which provided a small number of subjects preprocessed with many
pipelines, HBN-POD2 contains many subjects, all processed with a single state of
the art pipeline, \emph{QSIPrep}. For researchers developing new preprocessing
algorithms, HBN-POD2 provides a large, open baseline to which to compare their
results.

Similarly, HBN-POD2 will serve as a baseline for quality control methods.
HBN-POD2 provides four separate QC scores alongside its large dataset of
pediatric neuroimaging diffusion derivatives. Including these scores will help
pediatric mental health researchers improve inferences derived from the imaging
data, as suggested by the improvements in age prediction shown in Figure
\ref{fig:age-prediction}. In addition, neuroimaging QC methods developers will
benefit from a large benchmark dataset with which to pilot new QC methods.

\arhtodo[inline]{Add XGB-q QC scores to the participants.tsv on FCP-INDI}

We provide four different scores in anticipation of end-user's reluctance to
trust a QC score generated through deep learning \cite{lipton2017doctor,
salahuddin2022transparency, Zech2018-ki}. We addressed these concerns through
example-based explanations of the scores, with attribution maps calculated using
integrated gradients. However, we recognize that some researchers may still be
leery of the deep learning generated scores. We therefore provide
\begin{enumerate}
    \item the mean expert QC score for the 200 subjects in the gold standard
    dataset,
    \item the scores predicted by the XGB model, which outperformed all other models
    when evaluated against the gold standard ratings, but which are only available
    for participants that have community science scores,
    \item the scores predicted by the XGB-q model, which underperformed the deep
    learning generated scores, but which rely only on the automated QC metrics
    output by \emph{QSIPrep}, and may therefore be viewed as more interpretable.
    These are available for all participants.
\end{enumerate}
We view the XGB-q scores as a more interpretable fallback for users who are not
comfortable basing study exclusion decisions on a deep learning model
\cite{rudin2019stop}. The decision to use a more interpretable but slightly less
performant method of generating QC scores was also advocated by Tobe et
al.~\cite{tobe2021longitudinal}, who noted that the Euler number can reliably
predict braindr scores in the NKI-Rockland dataset.

We also note that the issue of algorithmic impact in choosing a QC method is not
exclusive to the deep learning model. We have chosen models that most reliably
reproduce the gold standard ratings, but a reliable algorithm might still
negatively influence researcher's decisions. For example, excluding participants
by QC score could spur them to exclude populations deserving of study, as when
QC score is highly correlates with age or socio-economic status. We therefore
caution researchers to examine interactions between the QC scores we provide and
their phenotype of interest.

\subsection*{Generalizing the hybrid QC approach to dMRI}

In this work, we have generalized our previous work on what we now call ``hybrid
QC''. This approach, originally applied to the HBN T1-weighted data
\cite{keshavan2019-er} makes use of a small expert-labeled data set, a larger
set of community scientist-labeled data and two steps of machine learning: the
first to match between the community scientists and the experts, and the second
to match between the images and the augmented community scientist labels.

This approach was generalized in several respects: first, from a single volume
of T1-weighted data to the multi-volume dMRI.

\arhtodo[inline]{%
    Add some text about the benefit of generalizing the hybrid approach to
    multisite and multi-modality data. Also need to discuss comparison with
    previous T1-weighted work. One thing to discuss there is that expert
    inter-rater reliability was *a lot* higher for T1-weighted. This is
    potentially a problem, but also highlights how important it is to have an
    automated, "objective" approach.
}

\rokemtodo[inline]{%
    Add some text about the benefit of generalizing the hybrid approach to
    multisite and multi-modality data.
}

\subsection*{Future work and open problems}

The HBN's plans to acquire imaging data for \num{10000} participants,
necessitating future releases. Future releases of HBN will require future
releases of HBN-POD2, plan for future releases is essential. This is a general
issue affecting multi-year neuroimaging projects for which derivative data is
being released before study completion. The use of \emph{QSIPrep},
\emph{cloudknot} and the containerization of the QC score assignment process
facilitate running the exact pipeline described in this paper on newly released
subjects. However, this approach is somewhat unsatisfactory because it fails to
anticipate improvements in preprocessing methodology. That is, what should we do
when \emph{QSIPrep} is inevitably updated between HBN releases: enforce
standardization by using an outdated pipeline or use state-of-the-art
preprocessing at the expense of standardized processing between releases?
Because the use of \emph{cloudknot} and AWS Spot Instances made preprocessing
relatively inexpensive, we propose a third way: if improvements to the
preprocessing pipeline are available with a new HBN release, we plan to execute
the improved pipeline on the entire HBN dataset, while preserving the previous
baseline release in an archived BIDS derivative dataset.

\rokemtodo[inline]{%
    After we run the site generalization experiements, speculate about
    generalization to other datasets. We should point out ABCD in particular
    since the CUNY and CBIC sites are supposed to be harmonized with ABCD. This
    discussion will hinge on the results of the remaining multi-site
    generalization experiments.
}

\section*{Methods}

To facilitate replicability, Jupyter notebooks \cite{kluyver2016jupyter} and
Dockerfiles \cite{merkel2014docker} necessary to reproduce the methods described
herein are provided in the HBN-POD2 GitHub repository at
\url{https://github.com/richford/hbn-pod2-qc}. The specific version of the
repository used in this study is also available at
\url{https://doi.org/our.awesome.doi}. The \texttt{make} or \texttt{make help}
commands will list the available commands and \texttt{make build} will build the
requisite Docker images to analyze HBN-POD2 QC data. In order to separate data
from analysis code \cite{Wilson2017-rj}, we provide intermediate data necessary
to analyze the QC results in an OSF \cite{Foster-MSLS2017-rl} project
\cite{hbn-pod2-osf}, which can be downloaded using the \texttt{make data}
command in the root of the HBN-POD2 GitHub repository. Most of the code in this
repository uses Pandas \cite{mckinney-proc-scipy-2010,reback2020pandas}, Numpy
\cite{harris2020array}, Matplotlib \cite{hunter2007matplotlib}, and Seaborn
\cite{waskom2021seaborn}.
\arhtodo[inline]{Zenodo the Github repo and add a citation here.}

Inputs for this study consisted of MRI data from the Healthy Brain Network
pediatric mental health study \cite{alexander2017-yc}, containing dMRI data from
\num{2747} subjects with ages 5-21. These data were measured using a
\qty{1.5}{\tesla} Siemens mobile scanner on Staten Island (SI) and three fixed
\qty{3}{\tesla} Siemens MRI scanners at sites in the New York area: Rutgers
University Brain Imaging Center (RUBIC), the CitiGroup Cornell Brain Imaging
Center (CBIC), and the City University of New York Advanced Science Research
Center (CUNY). Informed consent was obtained from each participant aged 18 or
older. For participants younger than 18, written consent was obtained from their
legal guardians and written assent was obtained from the participant. Voxel
resolution was \qty{1.8}{\mm} $\times$ \qty{1.8}{\mm} $\times$ \qty{1.8}{\mm} with \num{64} non-colinear
directions measured for each of $b=1000$ \unit{\second \per \mm^{2}} and
$b=2000$ \unit{\second \per \mm^{2}}.

\subsection*{BIDS curation}

We curated the imaging metadata for \num{2615} of the \num{2747} currently
available HBN subjects. Using dcm2bids and custom scripts, we conformed the data
to the Brain Imaging Data Structure (BIDS; \cite{gorgolewski2016-lh})
specification. The BIDS-curated dataset is available on FCP-INDI and can be
accessed via AWS S3 at \url{s3://fcp-indi/data/Projects/HBN/BIDS_curated/}.

\mctodo[inline]{Add more BIDS curation information}

\subsection*{Preprocessing}

We performed dMRI preprocessing on \num{2136} subjects, using \emph{QSIPrep}
\cite{cieslak2021-iq} 0.12.1, which is based on \emph{Nipype} 1.5.1
\cite{nipype1,nipype2}, RRID:SCR\_002502. \emph{QSIPrep} a robust and scalable
pipeline to group, distortion correct, motion correct, denoise, coregister and
resample MRI scans. In total, \num{417} subjects failed this preprocessing
step, largely due to missing dMRI files. In keeping with the BIDS specification,
the preprocessed dataset is available as a derivative dataset within the
BIDS-curated dataset and can be access on AWS S3 at
\url{s3://fcp-indi/data/Projects/HBN/BIDS_curated/derivatives/qsiprep/}.
\emph{QSIPrep} fosters reproducibility by automatically generating thorough
methods boilerplate for later use in scientific publications, which we use for
the remainder of this subsection to document each preprocessing step.

\begin{itemize}

\item {\it Anatomical data preprocessing}
The T1-weighted (T1w) image was corrected for intensity non-uniformity (INU)
using \texttt{N4BiasFieldCorrection} \cite{n4} (ANTs 2.3.1), and used as
T1w-reference throughout the workflow. The T1w-reference was then skull-stripped
using \texttt{antsBrainExtraction.sh} (ANTs 2.3.1), using OASIS as target
template. Spatial normalization to the ICBM 152 Nonlinear Asymmetrical template
version 2009c \cite{mni}, RRID:SCR\_008796 was performed through nonlinear
registration with \texttt{antsRegistration} \cite{ants}, ANTs 2.3.1,
RRID:SCR\_004757, using brain-extracted versions of both T1w volume and
template. Brain tissue segmentation of cerebrospinal fluid (CSF), white-matter
(WM) and gray-matter (GM) was performed on the brain-extracted T1w using
\texttt{FAST} \cite{fsl-fast}, FSL 6.0.3:b862cdd5, RRID:SCR\_002823.

\item {\it Diffusion data preprocessing}

Any images with a $b$-value less than \qty{100}{\second \per \mm^{2}} were treated
as a $b=0$ image. MP-PCA denoising as implemented in MRtrix3's
\texttt{dwidenoise}\cite{dwidenoise1} was applied with a 5-voxel window. After
MP-PCA, B1 field inhomogeneity was corrected using \texttt{dwibiascorrect} from
MRtrix3 with the N4 algorithm \cite{n4}. After B1 bias correction, the mean
intensity of the DWI series was adjusted so all the mean intensity of the $b=0$
images matched across eachseparate DWI scanning sequence.

FSL (version 6.0.3:b862cdd5)'s eddy was used for head motion correction
and Eddy current correction \cite{anderssoneddy}. Eddy was configured
with a \(q\)-space smoothing factor of 10, a total of 5 iterations, and
\num{1000} voxels used to estimate hyperparameters. A linear first level model
and a linear second level model were used to characterize Eddy
current-related spatial distortion. \(q\)-space coordinates were
forcefully assigned to shells. Field offset was attempted to be
separated from subject movement. Shells were aligned post-eddy. Eddy's
outlier replacement was run \cite{eddyrepol}. Data were grouped by
slice, only including values from slices determined to contain at least
\num{250} intracerebral voxels. Groups deviating by more than four standard
deviations from the prediction had their data replaced with imputed
values. Data was collected with reversed phase-encode blips, resulting
in pairs of images with distortions going in opposite directions. Here,
$b=0$ reference images with reversed phase encoding directions were used
along with an equal number of $b=0$ images extracted from the DWI scans.
From these pairs the susceptibility-induced off-resonance field was
estimated using a method similar to that described in \cite{topup}. The
fieldmaps were ultimately incorporated into the Eddy current and head
motion correction interpolation. Final interpolation was performed using
the \texttt{jac} method.

Several confounding time-series were calculated based on the
\emph{preprocessed DWI}: framewise displacement (FD) using the implementation
in \emph{Nipype} following the definitions by \cite{power-fd-dvars}. The DWI
time-series were resampled to ACPC, generating a \emph{preprocessed DWI run
in ACPC space}.

\end{itemize}

Many internal operations of \emph{QSIPrep} use \emph{Nilearn} 0.6.2
\cite{nilearn}, RRID:SCR\_001362 and \emph{DIPY} \cite{dipy}. For more details
of the pipeline, see
\href{https://qsiprep.readthedocs.io/en/latest/workflows.html}{the section
corresponding to workflows in \emph{QSIPrep}'s documentation}.

\subsection*{Cloud-based distributed preprocessing}

The containerization of \emph{QSIPrep} provided a consistent preprocessing
pipeline for each subject but the number of subjects made serial processing of
each particpant prohibitive on a single machine. We used \emph{cloudknot}, a
previously developed cloud-computing library \cite{cloudknot} to
parallelize the preprocessing over individual subjects on spot instances in the
Amazon Web Services Batch service. \emph{Cloudknot} takes as input a
user-defined Python function and creates the necessary AWS infrastructure to map
that function onto a range of inputs, in this case, the subject IDs. The Python
preprocessing function was a thin wrapper around \emph{QSIPrep}'s command line
interface and is provided in a jupyter notebook in the HBN-POD2 GitHub
repository in the ``notebooks'' directory. Using \emph{cloudknot} and AWS Batch
Spot Instances, the preprocessing cost less than \textdollar1.00 per subject.

\subsection*{Expert quality control}

The expert QC ``gold standard'' subset was created by randomly selecting 200
subjects from the preprocessed dataset, sampled such that the proportional site
distribution in the gold standard subset matched that of the preprocessed
dataset.

We created a web application for expert quality control of preprocessed dMRI,
called \emph{dmriprep-viewer} \cite{richie-halford2021-viewer}. The viewer
ingests \emph{QSIPrep} outputs and generates a browser-based interface for
expert QC. It provides a study overview page displaying the distributions of
\emph{QSIPrep}'s automated QC metrics (described at
\url{https://qsiprep.readthedocs.io/en/latest/preprocessing.html#quality-control-data}).
Each datum on the study overview page is interactively linked to a subject-level
QC page that provides an interactive version of \emph{QSIPrep}'s visual reports
(described at
\url{https://qsiprep.readthedocs.io/en/latest/preprocessing.html#visual-reports}).
The viewer allows users to assign a rating of $-2$ (definitely fail), $-1$
(probably fail), $0$ (not sure), $1$ (probably pass), or $2$ (definitely pass) to a
subject. To standardize rater expectations before rating, expert raters watched
a tutorial video (which is available in the OSF project). They then rated each
subject and saved their scores and sent them to the lead author for compilation.

\comment[inline]{%
    The new viewer will be called NIRV, the neuroimaging report viewer and we'll
    write another paper about that. So lots of details about the implementation
    are witheld from this paper.
}

To compute the pairwise Cohen's $\kappa$ scores in \ref{fig:expert-qc:irr}, we
used the \emph{scikit-learn} \cite{scikit-learn} \texttt{cohen\_kappa\_score}
function with quadratic weights. To compute intraclass correlation, we used the
\emph{pingouin} statistical package \cite{vallat2018pingouin}
\texttt{intraclass\_corr} function. The expert rating analysis can be replicated
using the \texttt{make expert-qc} command in the HBN-POD2 GitHub repository.

\subsection*{Community scientist quality control}

The community science web application is based on the SwipesForScience framework
\url{https://swipesforscience.org/}, which generates a web application for
community science given an open repository of images to be labelled and a
configuration file. The source code for the \emph{Fibr} web application is
available at \url{https://github.com/richford/fibr}. After a brief tutorial,
community scientists provided binary pass/fail ratings based on the DEC-FA from
a fit of a DTI model to each subject's preprocessed dMRI data. These images were
generated using a \emph{DIPY} \cite{dipy} \texttt{TensorModel} in a
\emph{cloudknot}-enabled Jupyter notebook that is available in the ``notebooks''
directory of the \emph{Fibr} GitHub repository. \emph{Fibr} saves each community
rating to its Google Firebase backend, the contents of which have been archived
to the HBN-POD2 OSF project.

The \emph{Fibr} ratings were then combined with automated \emph{QSIPrep} QC
metrics to train the gradient boosted trees models XGB, XGB-f, and XGB-q. These
models were implemented using the XGBoost library \cite{xgboost}. Using repeated
stratified K-fold cross-validation, with three splits and two repeats, we
evaluated the models' performance in predicting the gold standard ratings. In
each fold, the best model hyperparameters were chosen using the scikit-optimize
\cite{scikit-optimize} \texttt{BayesSearchCV} class. Saved model checkpoints for
each cross-validation split are available in the HBN-POD2 OSF project. Since
each split resulted in a different XGB model and we required a single QC score
to train the deep learning model, we combined the models from each
cross-validation split using a voting classifier, computing a weighted averaged
of the predicted probability of passing from each model, weighted by its
out-of-sample ROC AUC. This was implemented using scikit-learn's
\texttt{VotingClassifier} class. Treating the voting classifier as another
``expert'' rater, we reassessed the pairwise Cohen's $\kappa$ and ICC scores as
in the expert QC subsection. The community ratings analysis can be replicated
using the \texttt{make community-qc} command in the HBN-POD2 GitHub repository.

\subsection*{Deep learning to predict quality control}

\begin{figure}[tbp]
    \begin{subfigure}[t]{0.6\textwidth}
    \centering
    \includegraphics[width=\linewidth]{deep-learning-qc/model.pdf}
    \caption{Slicing and combining the input channels}
    \label{fig:dl-architecture:complete}
    \end{subfigure}
    \begin{subfigure}[t]{0.4\textwidth}
    \centering
    \includegraphics[width=\linewidth]{deep-learning-qc/image_model.pdf}
    \caption{CNN architecture}
    \label{fig:dl-architecture:cnn}
    \end{subfigure}
    \caption{%
        {\bf Deep learning model architecture}:
        \textbf{(a)} The CNN-i+q model accepts multichannel input that combined
        four imaging channels with a fifth channel containing 31 \emph{QSIPrep}
        automated QC metrics. The imaging channels are separated from the QC
        channel using \texttt{Lambda} layers. The imaging channels are passed
        through a CNN \textbf(b), the output of which is concatenated with the
        QC metrics, batch normalized and passed through two fully-connected (FC)
        layers, with rectified linear unit (ReLu) activation functions and with
        512 and 128 units respectively. Each FC layer is followed by a dropout
        layer which drops 40\% of the input units. The final layer contains a
        single unit with a sigmoid activation function and outputs the
        probability of passing QC.
        %
        \textbf{(b)} The CNN portion of the model passes the imaging input
        through four convolutional blocks. Each block consists of a 3D
        convolutional layer with a kernel size of 3 and a ReLu activation, a 3D
        max pooling layer with a pool size of 2, and a batch normalization layer
        with Tensorflow's default parameters. The number of filters in the
        convolutional layers in each block are 64, 64, 128, and 256 respecively.
        The output of the final block is passed through a 3D global average
        pooling layer with Tensorflow's default parameters.}
    \label{fig:dl-architecture}
\end{figure}

\arhtodo{The text in the caption: "The final layer contains a single unit with a
sigmoid activation function and outputs the probability of passing QC." seems in
contrast to the text below that suggests that the network predicts a continous
QC score, not a probability of a binary score, which is what this text suggests}


The voting classifier's predictions were then used as targets to train a deep
learning classifier to predict QC scores based on each subject's preprocessed
dMRI data. We trained two different model architectures
\begin{enumerate*}[%
    label=(\roman*),%
    before=\unskip{: },%
    itemjoin={{, }},%
    itemjoin*={{ and }}]
    \item CNN-i, which took only preprocessed dMRI images as input
    \item CNN-i+q, whose input also included \emph{QSIPrep}'s automated QC metrics.
\end{enumerate*}
Both models were implemented in Tensorflow 2 \cite{tensorflow} using the Keras
module \cite{keras}. The image processing part of the model architecture was
identical for both models: a modification of an existing 3D CNN
\cite{zunair2020-bs} previously applied to the assess tuberculosis severity
\cite{dicente2019clef}. It accepts a 3D volume as input with four channels
\begin{enumerate*}[%
    label=(\roman*),%
    before=\unskip{: },%
    itemjoin={{, }},%
    itemjoin*={{ and }}]
    \item the B=0 reference volume
    \item DEC-FA in the $x$-direction
    \item DEC-FA in the $y$-direction
    \item DEC-FA in the $z$-direction.
\end{enumerate*}
The \emph{QSIPrep}'s automated QC metrics were included as an additional fifth
channel. The CNN-i+q model architecture is summarized in Figure
\ref{fig:dl-architecture}. Upon input, the CNN-i+q model extracts the imaging
channels and passes them through the CNN architecture. The remaining QC metric
channel is flattened and passed ``around'' the CNN architecture and concatenated
with the output of the convolutional layers. This concatenated output is then
passed through a fully-connected layer to produce a single output, the
probability of passing QC. This architecture has 1,438,783 trainable parameters.

We used \emph{DIPY} \cite{dipy} and \emph{cloudknot} \cite{cloudknot} to
generate these multichannel volumes for each subject and save them as NIfTI-1
files \cite{nifti}. These NIfTI files were then converted to the Tensorflow
TFRecord format using the \emph{Nobrainer} deep learning framework
\cite{nobrainer}.
\arhtodo[inline]{%
    Before submission, ask the nobrainer folks to cut a new release that
    contains Adam's contributions so that we are referencing the correct
    version. This cites a previous version that will not accept multichannel
    volumes.
}
The Jupyter notebooks used to create these NIfTI and TFRecord files are
available in the ``notebooks'' directory of the \emph{Fibr} GitHub
repository.

We trained each model using the Google Cloud AI Platform Training service and
the HBN-POD2 GitHub repository contains Docker services to launch training (with
\texttt{make dl-train}) and prediction (with \texttt{make dl-predict}) jobs on
Google Cloud, if the user has provided the appropriate credentials in an
environment file and placed the TFRecord files on Google Cloud Storage. To
estimate the variability in model training, we trained ten separate models using
different training and validation splits of the data. The gold standard dataset
was not included in any of these splits and was reserved for reporting final
model performance. Models were optimized for binary crossentropy loss using the
Adam optimizer \cite{kingma2017adam} with an initial learning rate of 0.0001. We
reduced the learning rate by a factor of 0.5 when the validation loss plateaued
for more than two epochs. We also stopped training when the validation loss
failed to improve by more than 0.001 for twenty consecutive epochs. These two
adjustments were made using the \texttt{ReduceLROnPlateau} and
\texttt{EarlyStopping} callbacks in Tensorflow 2 \cite{tensorflow} respectively.
The training and validation loss curves for both the CNN-i and CNN-i+q models
are depicted in Figure \ref{fig:dl-loss}. While the CNN-i+q model achieved
better validation loss, it did not outperform the CNN-i model on the held out
gold standard dataset.

\begin{figure}[tbp]
    \begin{subfigure}[t]{0.5\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{deep-learning-qc/dl_learning_curve_with_qc.pdf}
    \caption{CNN-i+q}
    \label{fig:dl-loss:both}
    \end{subfigure}
    \begin{subfigure}[t]{0.5\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{deep-learning-qc/dl_learning_curve_without_qc.pdf}
    \caption{CNN-i}
    \label{fig:dl-loss:imaging}
    \end{subfigure}
    \caption{%
        {\bf Deep learning model loss curves}:
        The binary cross-entropy loss (top), accuracy (middle), and ROC AUC
        (bottom) for the CNN-i model (left) and the CNN-i+q model (right). Model
        performance typically plateaued after twenty epochs but was allowed
        continue until meeting the early stopping criterion.
    }
    \label{fig:dl-loss}
\end{figure}

To generate the attribution maps, we followed Tensorflow's integrated gradients
tutorial \cite{integrated-gradients-tutorial} with a black baseline image and
128 steps in the Riemann sum approximation of the integral (i.e.
\texttt{m\_steps = 128}). In the HBN-POD2 GitHub repository, we provide a Docker
service to compute integrated gradient attribution maps on Google Cloud, which
can be invoked using the \texttt{make dl-integrated-gradients} command.

\subsection*{QC bundle profiles}

To generate bundle profiles, reconstruction was performed using the
\emph{QSIprep} 0.12.1 preconfigured reconstruction workflow
\texttt{mrtrix\_multishell\_msmt}, modified to generate two million streamlines
rather than the default ten million. Multi-tissue fiber response functions were
estimated using the dhollander algorithm. FODs were estimated via constrained
spherical deconvolution (CSD, \cite{originalcsd, tournier2008csd}) using an
unsupervised multi-tissue method \cite{dhollander2019response,
dhollander2016unsupervised}. Reconstruction was done using MRtrix3
\cite{mrtrix3}. FODs were intensity-normalized using mtnormalize
\cite{mtnormalize}.

These tractograms were then used as input to the Python Automated Fiber
Quantification toolbox (pyAFQ) \cite{kruper2021evaluating}. Twenty-four major
tracts, which are enumerated in \ref{fig:qc-profiles:md}, were identified using
multiple criteria: streamlines are selected as candidates for inclusion in a
bundle of streamlines that represents a tract if they pass through known
inclusion ROIs and do not pass through exclusion ROIs \cite{Wakana2007-nw}. In
addition, a probabilistic atlas is used to exclude streamlines which are
unlikely to be part of a tract and to adjudicate in cases where a streamline
could belong to more than one tract \cite{Hua2008-di}. Each streamline is
resampled to 100 nodes and the robust mean at each location is calculated by
estimating the 3D covariance of the location of each node and excluding
streamlines that are more than 5 standard deviations from the mean location in
any node. Finally, a bundle profile of tissue properties in each bundle was
created by interpolating the value of MRI maps of these tissue properties to the
location of the nodes of the resampled streamlines designated to each bundle. In
each of 100 nodes, the values are summed across streamlines, weighting the
contribution of each streamline by the inverse of the mahalanobis distance of
the node from the average of that node across streamlines. This means that
streamlines that are more representative of the tract contribute more to the
bundle profile, relative to streamlines that are on the edge of the tract.

These processes create bundle profiles, in which diffusion measures are
quantified and averaged along twenty-four major fiber tracts. We retain only the
mean diffusivity (MD) and the fractional anisotropy (FA) from a diffusion
kurtosis imagin (DKI) model \cite{jensen2005-ta}, and impute missing bundles
using median imputation as implemented by \emph{scikit-learn}'s
\texttt{SimpleImputer} class. Because the HBN-POD2 bundle
profiles exhibit strong site effects \cite{richie-halford2021multidimensional},
we used the ComBat harmonization method to robustly adjust for site effects in
the tract profiles. Initially designed to correct for site effects in gene
expression studies \cite{Johnson2007-kl}, ComBat employs a parametric empirical
Bayes approach to adjust for batch effects and has since been applied to
multi-site cortical thickness measurements \cite{fortin2018-hk}, multi-site DTI
studies \cite{fortin2017-be}, and functional MRI data in the Adolescent Brain
Cognitive Development Study (ABCD) \cite{nielson2018detecting}. We rely on the
\emph{neurocombat\_sklearn} library \cite{neurocombat-sklearn}, to apply ComBat in
before plotting bundle profiles in Figure \ref{fig:qc-profiles:md} using
plotting functions from the AFQ-Insight package
\cite{richie-halford2019insight}. The bundle profile analysis can be replicated
using the \texttt{make bundle-profiles} command in the HBN-POD2 GitHub
repository.

\subsection*{Brain age prediction}

We evaluated the effect of varying the QC cutoff on model performance by
observing cross-validated $R^2$ values of gradient boosted trees models
implemented using XGBoost. The input feature space for each model consisted of
\num{4800} features per subject, comprising 100 nodes for each of MD and FA in
the twenty-four major tracts. We imputed missing bundles and harmonized the
different scanning sites as above. The XGBoost models' hyperparameters were
hand-tuned to values that have been performant in the authors' previous
experience. We log-transformed each participant's age before prediction using
\emph{scikit-learn}'s \texttt{TransformedTargetRegressor} class. For each value
of the QC cutoff between 0 and 0.95, in steps of 0.05, we computed the
cross-validated $R^2$ values using \emph{scikit-learn}'s
\texttt{cross\_val\_score} function with repeated K-fold cross-validation using
five folds and five repeats. Note that the practice of whole dataset imputation
before cross-validation permits the possibility of dataset leakage between train
and test splits. Similarly, sequential hand-tuning of hyperparameters can lead
to unexpected overfitting \cite{hosseini2020itried}. A detailed study of aging
and the white matter would need to correct for these transgressions. For this
study we deemed the risk of overfitting acceptable for the less fraught
demonstration of the effect of varying the QC cutoff.

\bibliography{hbn-pod2}

\section*{Acknowledgements}

We would like to thank Anisha Keshavan for useful discussions of community
science and web-based quality control and for her work on SwipesForScience. This
manuscript was prepared using a limited access dataset obtained from the Child
Mind Institute Biobank, The Healthy Brain Network dataset. This manuscript
reflects the views of the authors and does not necessarily reflect the opinions
or views of the Child Mind Institute. This work was supported via BRAIN Initiative grant 1RF1MH121868-01 from the National Institutes of Mental Health.

\alltodo[inline]{Add grant numbers.}

\section*{Author contributions statement}

The first author named is the corresponding author. The last two authors named
share senior authorship. The first two authors named share lead authorship. The
remaining authors are listed in alphabetical order, with the exception of
the Fibr Community Science Consortium, whose members provided community science
QC ratings.

\alltodo[inline]{Please add your initials here as appropriate}

We describe contributions to the paper using the CRediT taxonomy \cite{brand2015-vd,allen2014-oc}:
\begin{itemize}
    \item Conceptualization: A.R-H., A.R., T.S., and M.C.;
    \item Methodology: A.R-H. and A.R.;
    \item Software: A.R-H. and M.C.;
    \item Validation: ;
    \item Formal Analysis: A.R-H. and M.C.;
    \item Investigation: A.R-H. and M.C.;
    \item Resources: M.M.;
    \item Data curation: M.C. and L.A.;
    \item Writing  Original Draft: A.R-H. and A.R.;
    \item Writing  Review \& Editing: ;
    \item Visualization: A.R-H.;
    \item Supervision: A.R. and T.S.;
    \item Project Administration: A.R-H. and A.R.;
    \item Funding Acquisition: A.R. and T.S.
\end{itemize}

\section*{Additional information}

To include, in this order: \textbf{Accession codes} (where applicable);
\textbf{Competing interests} (mandatory statement).

The corresponding author is responsible for submitting a
\href{http://www.nature.com/srep/policies/index.html#competing}{competing
interests statement} on behalf of all authors of the paper. This statement must
be included in the submitted article file.

\arhtodo[inline]{Add accession codes and competing interests statement.}

% \begin{figure}[ht]
% \centering
% \includegraphics[width=\linewidth]{stream}
% \caption{Legend (350 words max). Example legend text.}
% \label{fig:stream}
% \end{figure}

% \begin{table}[ht]
% \centering
% \begin{tabular}{|l|l|l|}
% \hline
% Condition & n & p \\
% \hline
% A & 5 & 0.1 \\
% \hline
% B & 10 & 0.01 \\
% \hline
% \end{tabular}
% \caption{\label{tab:example}Legend (350 words max). Example legend text.}
% \end{table}

% Figures and tables can be referenced in LaTeX using the ref command, e.g. Figure \ref{fig:stream} and Table \ref{tab:example}.

\end{document}
